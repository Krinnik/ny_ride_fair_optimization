{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from geopy.geocoders import Nominatim\n",
    "from shapely.geometry import Point\n",
    "from shapely import wkb\n",
    "from shapely import errors\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "jan_app_df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-01.parquet').sample(frac=0.035, random_state=1)\n",
    "feb_app_df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-02.parquet').sample(frac=0.035, random_state=1)\n",
    "mar_app_df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-03.parquet').sample(frac=0.035, random_state=1)\n",
    "apr_app_df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-04.parquet').sample(frac=0.035, random_state=1)\n",
    "may_app_df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-05.parquet').sample(frac=0.035, random_state=1)\n",
    "jun_app_df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-06.parquet').sample(frac=0.035, random_state=1)\n",
    "jul_app_df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-07.parquet').sample(frac=0.035, random_state=1)\n",
    "aug_app_df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-08.parquet').sample(frac=0.035, random_state=1)\n",
    "sep_app_df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-09.parquet').sample(frac=0.035, random_state=1)\n",
    "oct_app_df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-10.parquet').sample(frac=0.035, random_state=1)\n",
    "nov_app_df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-11.parquet').sample(frac=0.035, random_state=1)\n",
    "dec_app_df = pd.read_parquet('https://d37ci6vzurychx.cloudfront.net/trip-data/fhvhv_tripdata_2024-12.parquet').sample(frac=0.035, random_state=1)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in only the columns and 10% of the samples\n",
    "\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "cab = pd.read_parquet(\"/Users/michaelbrady/Downloads/ny_taxi_2024_data.parquet\", columns=[\n",
    "    \"tpep_pickup_datetime\", \"tpep_dropoff_datetime\", \"trip_distance\", \"PULocationID\", \"DOLocationID\", \"fare_amount\",\n",
    "    \"tolls_amount\", \"Airport_fee\", \"congestion_surcharge\"]).sample(frac=0.1, random_state=1)\n",
    "cab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up target class for cab vs. ride share\n",
    "\n",
    "cab[\"class\"] = 0\n",
    "cab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above but with 1% of data (This is only one month of ride-share)\n",
    "\n",
    "fhv = pd.read_parquet(\"/Users/michaelbrady/Downloads/fhvhv_tripdata_2024-01.parquet\", columns=[\n",
    "    'pickup_datetime', 'dropoff_datetime', 'trip_miles', 'PULocationID', 'DOLocationID', \n",
    "    'base_passenger_fare', 'tolls', \"airport_fee\", 'congestion_surcharge', 'hvfhs_license_num']\n",
    "    ).sample(frac=0.50, random_state=1)\n",
    "fhv = fhv.rename(columns={'hvfhs_license_num': 'class'})\n",
    "fhv[\"class\"] = 1\n",
    "fhv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhv[\"pickup_datetime\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# match columns and concat\n",
    "\n",
    "fhv.columns = cab.columns\n",
    "df = pd.concat([cab, fhv], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"passenger_count\"] = df[\"passenger_count\"].fillna(1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df[\"passenger_count\"] = df[\"passenger_count\"].replace(0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"passenger_count\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"congestion_surcharge\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['congestion_surcharge'].isin([2.5, 0.0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"mta_tax\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quick, inprecise handling of outlier values\n",
    "\n",
    "#df = df[df[\"mta_tax\"].isin([0.50, 0.00])]\n",
    "df = df[df[\"trip_distance\"] < 100]\n",
    "df = df.loc[(df[\"PULocationID\"] < 264) & (df[\"DOLocationID\"] < 264)]\n",
    "df = df.loc[(df[\"fare_amount\"] > 0) & (df[\"fare_amount\"] < 300)]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIK: USE THESE FEATURES\n",
    "\n",
    "# converting raw datetime to features usable in most ML models\n",
    "\n",
    "df['time_diff_seconds'] = (df['tpep_dropoff_datetime'] - df['tpep_pickup_datetime']).dt.total_seconds()\n",
    "df['second_of_day'] = (\n",
    "    df['tpep_pickup_datetime'].dt.hour * 3600 + \n",
    "    df['tpep_pickup_datetime'].dt.minute * 60 +  \n",
    "    df['tpep_pickup_datetime'].dt.second)\n",
    "df['day_of_year'] = df['tpep_pickup_datetime'].dt.day_of_year\n",
    "\n",
    "# boolean features for holday and weekend categories\n",
    "\n",
    "df['is_weekend'] = df['tpep_pickup_datetime'].dt.weekday >= 5\n",
    "\n",
    "import holidays\n",
    "\n",
    "# US Holidays\n",
    "us_holidays = holidays.US()\n",
    "\n",
    "# Create a boolean holiday column\n",
    "df['is_holiday'] = df['tpep_pickup_datetime'].apply(lambda x: 1 if x.date() in us_holidays else 0)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"morning_rush_hour\"] = ((df[\"tpep_pickup_datetime\"].dt.weekday < 5) & \n",
    "                           (df[\"tpep_pickup_datetime\"].dt.hour.between(7, 9))).astype(int)\n",
    "df[\"evening_rush_hour\"] = ((df[\"tpep_pickup_datetime\"].dt.weekday < 5) & \n",
    "                           (df[\"tpep_pickup_datetime\"].dt.hour.between(16, 18))).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# check for correlations / collinearity\n",
    "\n",
    "corr_matrix = df.corr()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_df = pd.read_csv('https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv')\n",
    "\n",
    "taxi_zone_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_zone_df['service_zone'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in geospatial data\n",
    "\n",
    "zone_long_lat_data = pd.read_parquet('https://data.source.coop/cholmes/nyc-taxi-zones/taxi_zones_4326.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_long_lat_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zone_long_lat_data[\"borough\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed Nik's beautiful code\n",
    "\n",
    "def safe_wkb_loads(wkb_string):\n",
    "    try:\n",
    "        return wkb.loads(wkb_string)\n",
    "    except errors.WKTReadingError:\n",
    "        return Point(0,0)\n",
    "\n",
    "zone_long_lat_data['geometry'] = zone_long_lat_data['geometry'].apply(safe_wkb_loads)\n",
    "\n",
    "geo_zone = gpd.GeoDataFrame(zone_long_lat_data, geometry=zone_long_lat_data['geometry'], crs=\"EPSG:4326\")\n",
    "\n",
    "geo_zone_proj = geo_zone.to_crs(\"EPSG:3857\")\n",
    "\n",
    "geo_zone_proj['centroid'] = geo_zone_proj['geometry'].centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# NIK: USE THESE FEATURES\n",
    "# merging zone data w/ df for PU info\n",
    "\n",
    "zone_data = zone_long_lat_data[[\"LocationID\", \"borough\"]]\n",
    "zone_data = zone_data.copy()\n",
    "zone_data.loc[:, \"PULocationID\"] = zone_data[\"LocationID\"]\n",
    "df_w_zones = df.merge(zone_data, on=\"PULocationID\", how=\"left\")\n",
    "# f_w_zones = df_w_zones[df_w_zones[\"borough\"] == \"Manhattan\"]\n",
    "df_encoded = pd.get_dummies(zone_data['borough'], prefix=\"PU\")\n",
    "df1 = pd.concat([df_w_zones, df_encoded], axis=1)\n",
    "df1 = df1.drop(columns=[\"LocationID\", \"borough\"])\n",
    "df1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# merging zone data w/ df for DO info\n",
    "\n",
    "zone_data = zone_long_lat_data[[\"LocationID\", \"borough\"]]\n",
    "zone_data = zone_data.copy()  # Ensure you're working with a separate copy\n",
    "zone_data.loc[:, \"DOLocationID\"] = zone_data[\"LocationID\"]\n",
    "df1 = df.merge(zone_data, on=\"DOLocationID\", how=\"left\")\n",
    "df_encoded_2 = pd.get_dummies(zone_data['borough'], prefix=\"DO\")\n",
    "df1 = pd.concat([df1, df_encoded_2], axis=1)\n",
    "df1 = df1.drop(columns=[\"LocationID\", \"borough\"])\n",
    "df1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIK: USE THESE FEATURES\n",
    "# merging zone data w/ df for PU info + borough one-hot\n",
    "\n",
    "pu_data = zone_long_lat_data[[\"LocationID\", \"borough\"]].copy()\n",
    "pu_data.rename(columns={\"LocationID\": \"PULocationID\"}, inplace=True)\n",
    "pu_dummies = pd.get_dummies(pu_data[\"borough\"], prefix=\"PU\")\n",
    "pu_data = pd.concat([pu_data, pu_dummies], axis=1).drop(columns=[\"borough\"])\n",
    "df = df.merge(pu_data, on=\"PULocationID\", how=\"left\")\n",
    "df = df.drop(columns=\"PU_EWR\") #drop for one-hot\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIK: USE THESE FEATURES\n",
    "# merging zone data w/ df for DO info + borough one-hot\n",
    "\n",
    "do_data = zone_long_lat_data[[\"LocationID\", \"borough\"]].copy()\n",
    "do_data.rename(columns={\"LocationID\": \"DOLocationID\"}, inplace=True)\n",
    "do_dummies = pd.get_dummies(do_data[\"borough\"], prefix=\"DO\")\n",
    "do_data = pd.concat([do_data, do_dummies], axis=1).drop(columns=[\"borough\"])\n",
    "df = df.merge(do_data, on=\"DOLocationID\", how=\"left\")\n",
    "df = df.drop(columns=\"DO_EWR\") #drop for one-hot\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = geo_zone_proj\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pulling useful data out of \"geometry\" column\n",
    "\n",
    "gdf[\"centroid_x\"] = gdf.geometry.centroid.x\n",
    "gdf[\"centroid_y\"] = gdf.geometry.centroid.y\n",
    "gdf[\"area\"] = gdf.geometry.area\n",
    "gdf[\"perimeter\"] = gdf.geometry.length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limiting geometry features to centroids for now\n",
    "\n",
    "gdf = gdf.loc[:, [\"centroid_x\", \"centroid_y\", \"LocationID\"]]\n",
    "gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # NIK: USE THESE FEATURES\n",
    "# merging geospatial w/ df\n",
    "# note: these are not lat/long, they are another system, equally (if not better) for ML.\n",
    "\n",
    "gdf[\"PULocationID\"] = gdf[\"LocationID\"]\n",
    "df1 = df1.merge(gdf.rename(columns={\"centroid_x\": \"PUx\", \"centroid_y\": \"PUy\"}), \n",
    "                on=\"PULocationID\", how=\"left\")\n",
    "\n",
    "gdf[\"DOLocationID\"] = gdf[\"LocationID\"]\n",
    "df1 = df1.merge(gdf.rename(columns={\"centroid_x\": \"DOx\", \"centroid_y\": \"DOy\"}), \n",
    "                on=\"DOLocationID\", how=\"left\")\n",
    "\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df1 = df1.drop(columns=[\"LocationID_x\", \"LocationID_y\", \"PULocationID_x\", \"PULocationID_x\", \"PULocationID_y\", \"DOLocationID\"])\n",
    "\n",
    "df1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns for easier reference\n",
    "\n",
    "df1.columns = [[\"PUtime\", \"DOtime\", \"distance\", \"fare\", \"tolls\", \"airport\", \"congestion\", \"class\", \n",
    "    \"duration(sec)\", \"second_of_day\", \"day_of_year\", \"weekend\", \"holiday\", \"morning_rush\", \"evening_rush\",\n",
    "    \"PU_Bronx\", \"PU_Brooklyn\", \"PU_Manhattan\", \"PU_Queens\", \"PU_Staten Island\", \n",
    "    \"DO_Bronx\", \"DO_Brooklyn\", \"DO_Manhattan\", \"DO_Queens\", \"DO_Staten Island\", \n",
    "    \"PUx\", \"PUy\", \"DOx\", \"DOy\"]\n",
    "]\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NIK, Take a look at this example to asses similarities and differences\n",
    "\n",
    "# example of dataframe for ML modeling (prescaling, incomplete features, etc.)\n",
    "\n",
    "example_partial_unscaled_df_for_ML = df1[[\"second_of_day\", \"day_of_year\", \"weekend\", \"holiday\", \"PUx\", \"PUy\", \"DOx\", \"DOy\", \n",
    "                                          \"distance\", \"duration(sec)\", \"fare\", \"tolls\", \"airport\", \"congestion\", \"class\",  \"PU_Bronx\", \"PU_Brooklyn\", \n",
    "                                          \"PU_Manhattan\", \"PU_Queens\", \"PU_Staten Island\", \n",
    "                                          \"DO_Bronx\", \"DO_Brooklyn\", \"DO_Manhattan\", \"DO_Queens\", \"DO_Staten Island\", ]]\n",
    "example_partial_unscaled_df_for_ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "END OF REAL WORK - STOP READING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distances = []\n",
    "for (index1, row1), (index2, row2) in itertools.product(geo_zone_proj.iterrows(), geo_zone_proj.iterrows()):\n",
    "    distance_meters = row1['centroid'].distance(row2['centroid'])\n",
    "    distance_miles = distance_meters * 0.000621371\n",
    "    distances.append({\n",
    "        'PULocationID': index1,\n",
    "        'DOLocationID': index2,\n",
    "        'distance_miles': distance_miles\n",
    "    })\n",
    "\n",
    "distance_result_df = pd.DataFrame(distances)\n",
    "\n",
    "distance_result_df.tail(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(distance_result_df, on=['PULocationID', 'DOLocationID'], how='left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[500000:500020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['percentage_difference'] = (abs(df['trip_distance'] - df['distance_miles']) / df[['trip_distance', 'distance_miles']].max(axis=1)) * 100\n",
    "\n",
    "df_major_diff = df[df['percentage_difference'] > 50]\n",
    "\n",
    "df_major_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_distance = df[[\"trip_distance\", \"distance_miles\", \"time_diff_seconds\"]]\n",
    "df_distance.loc[:, \"trip_hours\"] = df[\"time_diff_seconds\"] / 60 / 60\n",
    "df_distance = df_distance.drop(columns=\"time_diff_seconds\")\n",
    "df_distance[\"mph_data\"] = df_distance[\"trip_distance\"] / df_distance[\"trip_hours\"]\n",
    "df_distance[\"mph_centroids\"] = df_distance[\"distance_miles\"] / df_distance[\"trip_hours\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cenroid distances won't work. Here's why:\n",
    "\n",
    "df_distance.loc[500000:500020]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(gdf)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=4) \n",
    "location_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Convert to DataFrame\n",
    "location_pca = pd.DataFrame(location_pca, columns=[\"PC1\", \"PCA\", \"PC3\", \"PC4\"])\n",
    "\n",
    "location_pca\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centroid_df = gdf.loc[:, [\"centroid_x\", \"centroid_y\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "centroid_scaled = scaler.fit_transform(centroid_df)\n",
    "\n",
    "pca2 = PCA(n_components=1) \n",
    "centroid_pca = pca2.fit_transform(centroid_scaled)\n",
    "\n",
    "centroid_pca = pd.DataFrame(centroid_pca, columns=[\"PC1\"])\n",
    "\n",
    "centroid_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca2.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "pass_impute = df1.drop(columns=[\"start_time\", \"end_time\", \"rating\"])\n",
    "pass_impute\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows_to_fill = pass_impute[\"pass_count\"].isnull()|(pass_impute[\"pass_count\"] == 0.0)\n",
    "rows_to_fill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pass_impute[~rows_to_fill]\n",
    "test_data = pass_impute[rows_to_fill]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.isnull().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(columns=[\"pass_count\", \"vendor\"])\n",
    "y = train_data[\"pass_count\"]\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = np.round(rf.predict(X_test))\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"MSE is {mse}\")\n",
    "print(f\"RMSE is {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred = np.round(rf.predict(X_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_mse = mean_squared_error(y_train, y_train_pred)\n",
    "t_rmse = np.sqrt(t_mse)\n",
    "print(f\"Training MSE is {t_mse}\")\n",
    "print(f\"Training RMSE is {t_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"trip_dist\", \"fare\", \"tip\", \"elapsed\"]\n",
    "\n",
    "\n",
    "pass_preds = np.round(rf.predict(test_data[features]))\n",
    "pass_impute.loc[rows_to_fill, \"pass_count\"] = pass_preds\n",
    "\n",
    "pass_impute\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_preds = pd.Series(pass_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_impute[\"pass_count\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = rf.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "sorted(zip(feature_names, importances), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop vendor column\n",
    "# Try adding start_time as hour and/or weekend column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pass_preds.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
