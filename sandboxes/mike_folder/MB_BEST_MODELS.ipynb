{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "\n",
    "df = pd.read_parquet(\"final_model_data.parquet\")\n",
    "\n",
    "# show all columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix a few row issues\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.columns = df.columns.str.replace(r\"[()',]\", \"\", regex=True).str.strip()\n",
    "df = df.replace({True: 1, False: 0})\n",
    "\n",
    "df.rename(columns={\"fore\": \"fare\"}, inplace=True)\n",
    "df.rename(columns={\"class\": \"service\"}, inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting to develop the idea for the target\n",
    "\n",
    "df[\"profit_per_hour\"] = df[\"fare\"] / (df[\"durationsec\"] / 3600).round(2)\n",
    "df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "df = df.loc[df[\"profit_per_hour\"] > 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers for more robust model\n",
    "\n",
    "df[\"profit_per_hour\"].std() * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"profit_per_hour\"] < 397.95]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create clusters with coordinates and airport data (experimental feature)\n",
    "\n",
    "pu_features = [\"PUx\", \"PUy\", \"airport\"]\n",
    "do_features = [\"DOx\", \"DOy\", \"airport\"]\n",
    "\n",
    "scaler_pu = StandardScaler()\n",
    "pu_scaled = scaler_pu.fit_transform(df[pu_features])\n",
    "\n",
    "scaler_do = StandardScaler()\n",
    "do_scaled = scaler_do.fit_transform(df[do_features])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find elbow for location and airport encoded clusters\n",
    "\n",
    "inertia = []\n",
    "K_range = range(7, 20) \n",
    "\n",
    "for k in K_range:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    kmeans.fit(pu_scaled)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(K_range, inertia)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SET K HERE\n",
    "\n",
    "pu_k = 9\n",
    "kmeans = KMeans(n_clusters=pu_k, random_state=42, n_init=10)\n",
    "df[\"cluster\"] = kmeans.fit_predict(pu_scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=df[\"PUx\"], y=df[\"PUy\"], hue=df[\"cluster\"], palette=\"tab10\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_taxi = df[(df[\"service\"] == 0) & (df[\"fare\"] > 0)]\n",
    "df_uber = df[(df[\"service\"] == 1) & (df[\"fare\"] > 0)]\n",
    "df_lyft = df[(df[\"service\"] == 2) & (df[\"fare\"] > 0)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uber.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn the clusters and service into booleans for training\n",
    "\n",
    "df = pd.get_dummies(df, columns=[\"cluster\"])\n",
    "df = pd.get_dummies(df, columns=[\"service\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop taxis, from here on my models use uber and lyft data only\n",
    "\n",
    "df = df[df[\"service_0\"] == 0]\n",
    "df[\"service_1\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to a geopandas df\n",
    "\n",
    "gdf = gpd.GeoDataFrame(df, geometry=gpd.points_from_xy(df[\"PUx\"], df[\"PUy\"]), crs=\"EPSG:2263\")  # Adjust EPSG if needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "kde = sns.kdeplot(\n",
    "    x=gdf.geometry.x, \n",
    "    y=gdf.geometry.y, \n",
    "    ax=ax, \n",
    "    fill=True, \n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# add basemap\n",
    "ctx.add_basemap(ax, source=ctx.providers.CartoDB.Positron, crs=gdf.crs)\n",
    "\n",
    "# EPSG instead of actual long/lat\n",
    "ax.set_title(\"NYC Driver Earnings Heatmap ($/hr)\")\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "\n",
    "# fix colorbar issue\n",
    "fig.colorbar(ax.collections[-1], ax=ax, label=\"Earnings per Hour ($)\")\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't rewrite over final parquet, commented out on purpose\n",
    "\n",
    "#df.to_parquet(\"final_model_data.parquet\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full feature modeling (minus temp and PU_Bronx)\n",
    "\n",
    "cluster_features = [col for col in df.columns if col.startswith(\"cluster_\")]\n",
    "\n",
    "features = [\n",
    "    \n",
    "    \"second_of_day\",\n",
    "    \"day_of_year\",\n",
    "    \"morning_rush\",\n",
    "    \"evening rush\",\n",
    "    \"temp\",\n",
    "    \"holiday\",\n",
    "    \"weekend\",\n",
    "    \"airport\",\n",
    "    \"congestion\",\n",
    "    \"PUx\",\n",
    "    \"PUy\",\n",
    "    \"DOx\",\n",
    "    \"DOy\",\n",
    "    \"PU_Brooklyn\",\n",
    "    \"PU_Manhattan\",\n",
    "    \"PU_Queens\",\n",
    "    \"PU_Staten Island\",\n",
    "    \"service_1\",\n",
    "] + cluster_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train duration model with full features\n",
    "\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "y = df[\"durationsec\"] \n",
    "X = df[features]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb.XGBRegressor(random_state=42), param_distributions=param_dist, n_iter=10, cv=3, n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "y_pred_xgb = random_search.best_estimator_.predict(X_test)\n",
    "\n",
    "xgb_duration = random_search.best_estimator_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_duration = random_search.best_estimator_\n",
    "\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost MAE: {mae_xgb:.2f}\")\n",
    "r2_via_metric = r2_score(y_test, y_pred_xgb)\n",
    "print(\"RÂ² via r2_score:\", r2_via_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assess for feature selection\n",
    "\n",
    "feature_importances = xgb_duration.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": feature_importances.round(2)\n",
    "})\n",
    "\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit model with features greater than 0.01\n",
    "\n",
    "features2 = importance_df[importance_df[\"Importance\"] > 0.01][\"Feature\"].tolist()\n",
    "features2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train duration model with reduced features\n",
    "\n",
    "y = df[\"durationsec\"] \n",
    "X = df[features2]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [300, 500],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [9, 13, 15],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb.XGBRegressor(random_state=42), param_distributions=param_dist, n_iter=10, cv=3, n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "y_pred_xgb = random_search.best_estimator_.predict(X_test)\n",
    "\n",
    "xgb_duration = random_search.best_estimator_\n",
    "\n",
    "\n",
    "xgb_duration = random_search.best_estimator_\n",
    "\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost MAE: {mae_xgb:.2f}\")\n",
    "r2_via_metric = r2_score(y_test, y_pred_xgb)\n",
    "print(\"RÂ² via r2_score:\", r2_via_metric)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train duration model with original features from other notebook\n",
    "\n",
    "features = [\"second_of_day\", \"day_of_year\", \"PUx\", \"PUy\", \"DOx\", \"DOy\", \"distance\", \"morning_rush\", \"evening rush\",\n",
    "            \"prcp\", \"temp\", \"holiday\", \"weekend\", \"airport\", \"congestion\", \"PU_Bronx\", \"PU_Brooklyn\",\n",
    "            \"PU_Manhattan\", \"PU_Queens\", \"PU_Staten Island\", \"DO_Bronx\", \"DO_Brooklyn\", \n",
    "            \"DO_Manhattan\", \"DO_Queens\", \"DO_Staten Island\"]\n",
    "\n",
    "y = df[\"durationsec\"] \n",
    "X = df[features]\n",
    "\n",
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': [300, 500],\n",
    "    'learning_rate': [0.01, 0.02],\n",
    "    'max_depth': [9, 13],\n",
    "    'subsample': [0.8],\n",
    "    'colsample_bytree': [0.4, 0.6],\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(xgb.XGBRegressor(random_state=42), param_distributions=param_dist, n_iter=10, cv=3, n_jobs=-1, random_state=42)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "y_pred_xgb = random_search.best_estimator_.predict(X_test)\n",
    "\n",
    "xgb_duration2 = random_search.best_estimator_\n",
    "\n",
    "\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost MAE: {mae_xgb:.2f}\")\n",
    "r2_via_metric = r2_score(y_test, y_pred_xgb)\n",
    "print(\"RÂ² via r2_score:\", r2_via_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best Duration Model of .86 RÂ² and about three minutes off (mean absolute error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train earnings model with full features\n",
    "\n",
    "X = df[features]\n",
    "y = df[\"profit_per_hour\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "xgb_model = xgb.XGBRegressor(random_state=3)\n",
    "\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": [300, 500],\n",
    "    \"max_depth\": [13, 15],\n",
    "    \"learning_rate\": [0.02, 0.05, 0.1],\n",
    "    \"subsample\": [0.6, 0.8],\n",
    "    \"colsample_bytree\": [0.4, 0.6]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_distributions=params,\n",
    "        n_iter=10,  \n",
    "        cv=3,  \n",
    "        n_jobs=-1, \n",
    "        random_state=3\n",
    "    )\n",
    "\n",
    "random_search.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model = random_search.best_estimator_\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(f\"RÂ² score: {r2}\")\n",
    "print(f\"Best parameters are: {random_search.best_params_}\")\n",
    "print(f\"These predictions are off by about ${mae:.2f}\")\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dummy regressor for baseline #1\n",
    "\n",
    "# Create a naive baseline model that always predicts the mean of y_train\n",
    "dummy_model = DummyRegressor(strategy=\"mean\")\n",
    "dummy_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict using the naive baseline model\n",
    "y_pred_dummy = dummy_model.predict(X_test)\n",
    "\n",
    "# Evaluate the naive model\n",
    "mae_dummy = mean_absolute_error(y_test, y_pred_dummy)\n",
    "r2_dummy = r2_score(y_test, y_pred_dummy)\n",
    "\n",
    "# Evaluate the trained XGBoost model\n",
    "y_pred_xgb = xgb_model.predict(X_test)  # Ensure you are using the best trained model\n",
    "mae_xgb = mean_absolute_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "\n",
    "# Print comparison results\n",
    "print(f\"ðŸ“Š Naive Baseline MAE: {mae_dummy:.2f}, RÂ²: {r2_dummy:.4f}\")\n",
    "print(f\"ðŸš€ XGBoost Model MAE: {mae_xgb:.2f}, RÂ²: {r2_xgb:.4f}\")\n",
    "\n",
    "# Determine if the model beats the baseline\n",
    "if mae_xgb < mae_dummy:\n",
    "    print(\"âœ… The XGBoost model performs better than the naive baseline!\")\n",
    "else:\n",
    "    print(\"âŒ The XGBoost model does NOT outperform the naive baseline.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking feature importances\n",
    "\n",
    "feature_importances = xgb_model.feature_importances_\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    \"Feature\": X.columns,\n",
    "    \"Importance\": feature_importances.round(2)\n",
    "})\n",
    "\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refit model with features greater than 0.01\n",
    "\n",
    "features2 = importance_df[importance_df[\"Importance\"] > 0.01][\"Feature\"].tolist()\n",
    "features2 = df[features2].drop(columns=\"distance\")\n",
    "features2.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the simpler model, zeroing in on params\n",
    "\n",
    "# Prepare data\n",
    "X = df[features2]\n",
    "y = df[\"profit_per_hour\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)\n",
    "\n",
    "params = {\n",
    "    \"n_estimators\": [800],\n",
    "    \"max_depth\": [15],\n",
    "    \"learning_rate\": [0.02],\n",
    "    \"subsample\": [0.8],\n",
    "    \"colsample_bytree\": [0.6],\n",
    "    \"reg_alpha\": [0.05, 0.1] \n",
    "}\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective=\"reg:squarederror\", random_state=3)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=params,\n",
    "    n_iter=8,  \n",
    "    cv=3,  \n",
    "    n_jobs=12, \n",
    "    random_state=3\n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# get best model\n",
    "xgb_model = random_search.best_estimator_\n",
    "\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"RÂ² score: {r2:.4f}\")\n",
    "print(f\"Best parameters are: {random_search.best_params_}\")\n",
    "print(f\"These predictions are off by about ${mae:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# whoops, so I won't save this model. distance left in - heavy leakage...\n",
    "\n",
    "features2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# don't save over best models unless intentional\n",
    "\n",
    "\"\"\"\n",
    "import pickle\n",
    "\n",
    "with open(\"duration_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(xgb_duration2, f)\n",
    "\n",
    "# Save the earnings per hour model\n",
    "with open(\"earnings_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(xgb_model, f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far best model performance is RÂ² of .29 and MAE of $17.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying an MLP regressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "nn_model = MLPRegressor(\n",
    "    hidden_layer_sizes=(32, 16),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    max_iter=100,\n",
    "    early_stopping=True,\n",
    "    verbose=True,\n",
    "    random_state=3\n",
    ")\n",
    "\n",
    "nn_model.fit(X_train_scaled, y_train)\n",
    "y_pred_nn = nn_model.predict(X_test_scaled)\n",
    "\n",
    "mae_nn = mean_absolute_error(y_test, y_pred_nn)\n",
    "print(f\"ðŸ§  Neural Network MAE (with Scaler): ${mae_nn:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"temp\"] == df[\"temp\"].max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
